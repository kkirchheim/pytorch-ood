"""

"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

from ..loss.crossentropy import cross_entropy
from ..utils import apply_reduction, is_known, is_unknown, evaluate_energy_logistic_loss

from typing import Callable, Tuple, Any
from numpy import floating


class EnergyMarginLoss(nn.Module):
    """
    Introducing Margin to further improve performance Energy-based OOD detection method,
    specifically for handling covariate shifted data.

    :see Paper:
        `arxiv <https://arxiv.org/pdf/2306.09158>`__

    :see Implementation: `GitHub <https://github.com/deeplearning-wisc/scone>`__
    """

    def __init__(
        self,
        full_train_loss: floating[Any],
        eta=1.00,
        false_alarm_cutoff=0.05,
        in_constraint_weight=1.00,
        ce_tol=2.00,
        ce_constraint_weight=1.00,
        out_constraint_weight=1.00,
        lr_lam=1.00,
        penalty_mult=1.50,
        constraint_tol=0.00
    ):
        super(EnergyMarginLoss, self).__init__()
        self.full_train_loss = torch.tensor(full_train_loss).float()
        self.eta = torch.tensor(eta).float()
        self.false_alarm_cutoff = torch.tensor(false_alarm_cutoff).float()
        self.in_constraint_weight = torch.tensor(in_constraint_weight).float()
        self.lam = torch.tensor(0).float()
        self.lam2 = torch.tensor(0).float()
        self.ce_tol = torch.tensor(ce_tol).float()
        self.ce_constraint_weight = torch.tensor(ce_constraint_weight).float()
        self.out_constraint_weight = torch.tensor(out_constraint_weight).float()
        self.lr_lam = torch.tensor(lr_lam).float()
        self.penalty_mult = torch.tensor(penalty_mult).float()
        self.constraint_tol = torch.tensor(constraint_tol).float()

    def forward(self, logits: torch.Tensor, targets: torch.Tensor, logistic_regression: Callable[[torch.Tensor], torch.Tensor]) -> torch.Tensor:
        """
        Calculates weighted sum of cross-entropy and the energy regularization term.

        :param logits: logits
        :param targets: labels
        :param logistic_regression: logistic regression layer
        """
        # for classification
        if len(logits.shape) == 2:
            energy_loss_in, energy_loss_out = self._sigmoid_loss(logits=logits, y=targets,logistic_regression=logistic_regression)
            loss_in = self._alm_in_distribution_constraint(energy_loss_in=energy_loss_in)
            loss_ce = F.cross_entropy(logits[is_known(targets)], targets[is_known(targets)])
            loss_ce = self._alm_cross_entropy_constraint(loss_ce=loss_ce)
        else:
            raise ValueError(f"Unsupported input shape: {logits.shape}")
        return apply_reduction(loss_ce + self.out_constraint_weight * energy_loss_out + loss_in, reduction=None)

    def _sigmoid_loss(
        self, logits: torch.Tensor, y: torch.Tensor, logistic_regression: Callable[[torch.Tensor], torch.Tensor]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        # for classification
        known = is_known(y)
        
        energy_loss_in = torch.mean(torch.sigmoid(logistic_regression(
            (torch.logsumexp(logits[is_known(y)], dim=1)).unsqueeze(1)).squeeze()))
        energy_loss_out = torch.mean(torch.sigmoid(-logistic_regression(
            (torch.logsumexp(logits[is_unknown(y)], dim=1) - self.eta).unsqueeze(1)).squeeze()))
        return energy_loss_in, energy_loss_out

    def _alm_in_distribution_constraint(self, energy_loss_in: torch.Tensor) -> torch.Tensor:
        # for classification
        in_constraint_term = energy_loss_in - self.false_alarm_cutoff
        
        if self.in_constraint_weight * in_constraint_term + self.lam >= 0:
            in_loss = in_constraint_term * self.lam + self.in_constraint_weight / 2 * torch.pow(in_constraint_term, 2)
        else:
            in_loss = - torch.pow(self.lam, 2) * 0.5 / self.in_constraint_weight
        return in_loss

    def _alm_cross_entropy_constraint(self, loss_ce: torch.Tensor) -> torch.Tensor:
        # for classification
        loss_ce_constraint = loss_ce - self.ce_tol * self.full_train_loss
        
        if self.ce_constraint_weight * loss_ce_constraint + self.lam2 >= 0:
            loss_ce = loss_ce_constraint * self.lam2 + self.ce_constraint_weight / 2 * torch.pow(loss_ce_constraint, 2)
        else:
            loss_ce = - torch.pow(self.lam2, 2) * 0.5 / self.ce_constraint_weight
        return loss_ce
    
    def update_hyperparameters(
        self, model: Callable[[torch.Tensor], torch.Tensor], train_loader_in: DataLoader, logistic_regression: Callable[[torch.Tensor], torch.Tensor]
    ) -> None:
        avg_sigmoid_energy_losses, _, avg_ce_loss = evaluate_energy_logistic_loss(model, train_loader_in, logistic_regression)
        
        # update lam
        in_term_constraint = avg_sigmoid_energy_losses -  self.false_alarm_cutoff
        if in_term_constraint * self.in_constraint_weight + self.lam >= 0:
            self.lam += self.lr_lam * in_term_constraint
        else:
            self.lam += -self.lr_lam * self.lam / self.in_constraint_weight
            
        # update lam2
        ce_constraint = avg_ce_loss - self.ce_tol * self.full_train_loss
        if ce_constraint * self.ce_constraint_weight + self.lam2 >= 0:
            self.lam2 += self.lr_lam * ce_constraint
        else:
            self.lam2 += -self.lr_lam * self.lam2 / self.ce_constraint_weight
        
        # update in-distribution weight for alm
        if in_term_constraint > self.constraint_tol:
            self.in_constraint_weight *= self.penalty_mult
            
        if ce_constraint > self.constraint_tol:
            self.ce_constraint_weight *= self.penalty_mult